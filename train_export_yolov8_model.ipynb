{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ebf4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df3ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27dd71-2b43-48bf-9f05-eba66c1afe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
    "\n",
    "# # Train the model with 2 GPUs\n",
    "results = model.train(data='data.yaml', epochs=100, imgsz=640 , device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5bf496c-65c0-4bc8-b076-e626ad5d87cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.24 üöÄ Python-3.11.6 torch-2.1.2+cu118 CUDA:0 (NVIDIA GeForce RTX 3090, 24576MiB)\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\yolov8_env\\ultralytics_android_app\\step_1_train_test_export\\dataset\\val\\labels.cac\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è E:\\yolov8_env\\ultralytics_android_app\\step_1_train_test_export\\dataset\\val\\images\\8YGAV61FP6DX_jpg.rf.58da95b585c9603f5f940e73eff64a57.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0132]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è E:\\yolov8_env\\ultralytics_android_app\\step_1_train_test_export\\dataset\\val\\images\\CK5FP969OKVT_jpg.rf.39129739a321267395ceb3dc451f0d26.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0011]\n",
      "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 20, len(boxes) = 149. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        116        149      0.827       0.53      0.641      0.467\n",
      "                  fork        116         46      0.575      0.652      0.587      0.359\n",
      "                 knife        116          4          1          0      0.278      0.112\n",
      "                 plate        116         11      0.981      0.636      0.859      0.746\n",
      "                 spoon        116         88      0.753      0.832      0.841       0.65\n",
      "Speed: 0.4ms preprocess, 6.9ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([     0.3591,     0.11213,     0.74554,     0.65032])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate model\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('runs/detect/train3/weights/best.pt')  # load a custom model\n",
    "\n",
    "# Validate the model\n",
    "metrics = model.val()  # no arguments needed, dataset and settings remembered\n",
    "metrics.box.map    # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps   # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28af2e54-9d78-4da5-8fa7-137344fcfd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/3 E:\\yolov8_env\\ultralytics_android_app\\step_1_train_test_export\\test_images\\a.jpg: 640x640 2 forks, 2 spoons, 20.0ms\n",
      "image 2/3 E:\\yolov8_env\\ultralytics_android_app\\step_1_train_test_export\\test_images\\b.jpg: 640x640 3 forks, 2 spoons, 38.0ms\n",
      "image 3/3 E:\\yolov8_env\\ultralytics_android_app\\step_1_train_test_export\\test_images\\d.jpg: 640x416 2 forks, 2 spoons, 17.0ms\n",
      "Speed: 3.0ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Results saved to \u001b[1mruns\\detect\\predict2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'fork', 1: 'knife', 2: 'plate', 3: 'spoon'}\n",
       " obb: None\n",
       " orig_img: array([[[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         ...,\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240]],\n",
       " \n",
       "        [[240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         ...,\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240]],\n",
       " \n",
       "        [[240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         ...,\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240],\n",
       "         [240, 240, 240]]], dtype=uint8)\n",
       " orig_shape: (512, 512)\n",
       " path: 'E:\\\\yolov8_env\\\\ultralytics_android_app\\\\step_1_train_test_export\\\\test_images\\\\a.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs\\\\detect\\\\predict2'\n",
       " speed: {'preprocess': 3.0007362365722656, 'inference': 19.999265670776367, 'postprocess': 2.0003318786621094},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'fork', 1: 'knife', 2: 'plate', 3: 'spoon'}\n",
       " obb: None\n",
       " orig_img: array([[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]], dtype=uint8)\n",
       " orig_shape: (500, 500)\n",
       " path: 'E:\\\\yolov8_env\\\\ultralytics_android_app\\\\step_1_train_test_export\\\\test_images\\\\b.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs\\\\detect\\\\predict2'\n",
       " speed: {'preprocess': 3.000497817993164, 'inference': 37.999629974365234, 'postprocess': 1.9989013671875},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'fork', 1: 'knife', 2: 'plate', 3: 'spoon'}\n",
       " obb: None\n",
       " orig_img: array([[[217, 217, 217],\n",
       "         [235, 235, 235],\n",
       "         [229, 229, 229],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[231, 231, 231],\n",
       "         [252, 252, 252],\n",
       "         [249, 249, 249],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[229, 229, 229],\n",
       "         [253, 253, 253],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         ...,\n",
       "         [252, 252, 252],\n",
       "         [247, 247, 247],\n",
       "         [243, 243, 243]],\n",
       " \n",
       "        [[230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         ...,\n",
       "         [246, 246, 246],\n",
       "         [241, 241, 241],\n",
       "         [238, 238, 238]],\n",
       " \n",
       "        [[230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         ...,\n",
       "         [237, 237, 237],\n",
       "         [231, 231, 231],\n",
       "         [228, 228, 228]]], dtype=uint8)\n",
       " orig_shape: (480, 300)\n",
       " path: 'E:\\\\yolov8_env\\\\ultralytics_android_app\\\\step_1_train_test_export\\\\test_images\\\\d.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs\\\\detect\\\\predict2'\n",
       " speed: {'preprocess': 2.9969215393066406, 'inference': 17.001628875732422, 'postprocess': 1.9989013671875}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Prediction using trained model\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLOv8n model\n",
    "model = YOLO('runs/detect/train3/weights/best.pt')\n",
    "\n",
    "# Run inference \n",
    "model.predict('test_images', save=True, imgsz=640, conf=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3d815e-d7ad-4f2d-b540-b3039f1230e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.62  Python-3.12.4 torch-2.3.1+cpu CPU (AMD Ryzen 7 5800H with Radeon Graphics)\n",
      "YOLOv8n summary (fused): 168 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'D:\\My-all-programs--\\Semester-4-Summer\\Inotech Intern\\Yolo-Object Detection in Android\\Object-Detection-Android-App\\yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['tensorflow-cpu>=2.0.0'] not found, attempting AutoUpdate...\n",
      "Collecting tensorflow-cpu>=2.0.0\n",
      "  Downloading tensorflow_cpu-2.17.0-cp312-cp312-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting tensorflow-intel==2.17.0 (from tensorflow-cpu>=2.0.0)\n",
      "  Downloading tensorflow_intel-2.17.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\shafeenyousafzai\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\shafeenyousafzai\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (24.3.25)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading h5py-3.11.0-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\shafeenyousafzai\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\shafeenyousafzai\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\shafeenyousafzai\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\shafeenyousafzai\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\shafeenyousafzai\\.conda\\envs\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shafeenyousafzai\\.conda\\envs\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\shafeenyousafzai\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\shafeenyousafzai\\.conda\\envs\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading grpcio-1.65.1-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.2.0 (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading keras-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\shafeenyousafzai\\.conda\\envs\\venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\shafeenyousafzai\\.conda\\envs\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (0.43.0)\n",
      "Collecting rich (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading optree-0.12.1-cp312-cp312-win_amd64.whl.metadata (48 kB)\n",
      "     -------------------------------------- 48.7/48.7 kB 144.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shafeenyousafzai\\.conda\\envs\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shafeenyousafzai\\.conda\\envs\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shafeenyousafzai\\.conda\\envs\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shafeenyousafzai\\.conda\\envs\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (2024.7.4)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\shafeenyousafzai\\.conda\\envs\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\shafeenyousafzai\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow-cpu>=2.0.0)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow_cpu-2.17.0-cp312-cp312-win_amd64.whl (2.1 kB)\n",
      "Downloading tensorflow_intel-2.17.0-cp312-cp312-win_amd64.whl (385.2 MB)\n",
      "   -------------------------------------- 385.2/385.2 MB 242.8 kB/s eta 0:00:00\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 57.5/57.5 kB 1.0 MB/s eta 0:00:0001\n",
      "Downloading grpcio-1.65.1-cp312-cp312-win_amd64.whl (4.1 MB)\n",
      "   ---------------------------------------- 4.1/4.1 MB 521.9 kB/s eta 0:00:00\n",
      "Downloading h5py-3.11.0-cp312-cp312-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 3.0/3.0 MB 463.3 kB/s eta 0:00:00\n",
      "Downloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 1.1/1.1 MB 789.0 kB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 26.4/26.4 MB 1.6 MB/s eta 0:00:0004\n",
      "Downloading tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 5.5/5.5 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-1.16.0-cp312-cp312-win_amd64.whl (37 kB)\n",
      "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "   ---------------------------------------- 105.4/105.4 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "   ---------------------------------------- 227.3/227.3 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.12.1-cp312-cp312-win_amd64.whl (267 kB)\n",
      "   -------------------------------------- 267.2/267.2 kB 513.9 kB/s eta 0:00:00\n",
      "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "   ---------------------------------------- 240.7/240.7 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "   ---------------------------------------- 87.5/87.5 kB 709.1 kB/s eta 0:00:00\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, wrapt, werkzeug, termcolor, tensorboard-data-server, optree, mdurl, markdown, h5py, grpcio, google-pasta, gast, astunparse, tensorboard, markdown-it-py, rich, keras, tensorflow-intel, tensorflow-cpu\n",
      "Successfully installed astunparse-1.6.3 gast-0.6.0 google-pasta-0.2.0 grpcio-1.65.1 h5py-3.11.0 keras-3.4.1 libclang-18.1.1 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 namex-0.0.8 optree-0.12.1 rich-13.7.1 tensorboard-2.17.0 tensorboard-data-server-0.7.2 tensorflow-cpu-2.17.0 tensorflow-intel-2.17.0 termcolor-2.4.0 werkzeug-3.0.3 wrapt-1.16.0\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success  787.1s, installed 1 package: ['tensorflow-cpu>=2.0.0']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['tf_keras', 'sng4onnx>=1.0.1', 'onnx_graphsurgeon>=0.3.26', 'onnx>=1.12.0', 'onnx2tf>1.17.5,<=1.22.3', 'onnxslim>=0.1.31', 'tflite_support', 'onnxruntime'] not found, attempting AutoUpdate...\n",
      "Retry 1/2 failed: Command 'pip install --no-cache-dir \"tf_keras\" \"sng4onnx>=1.0.1\" \"onnx_graphsurgeon>=0.3.26\" \"onnx>=1.12.0\" \"onnx2tf>1.17.5,<=1.22.3\" \"onnxslim>=0.1.31\" \"tflite_support\" \"onnxruntime\" --extra-index-url https://pypi.ngc.nvidia.com' returned non-zero exit status 1.\n",
      "Retry 2/2 failed: Command 'pip install --no-cache-dir \"tf_keras\" \"sng4onnx>=1.0.1\" \"onnx_graphsurgeon>=0.3.26\" \"onnx>=1.12.0\" \"onnx2tf>1.17.5,<=1.22.3\" \"onnxslim>=0.1.31\" \"tflite_support\" \"onnxruntime\" --extra-index-url https://pypi.ngc.nvidia.com' returned non-zero exit status 1.\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  Command 'pip install --no-cache-dir \"tf_keras\" \"sng4onnx>=1.0.1\" \"onnx_graphsurgeon>=0.3.26\" \"onnx>=1.12.0\" \"onnx2tf>1.17.5,<=1.22.3\" \"onnxslim>=0.1.31\" \"tflite_support\" \"onnxruntime\" --extra-index-url https://pypi.ngc.nvidia.com' returned non-zero exit status 1.\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.17.0...\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export failure  1038.5s: No module named 'onnx2tf'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'onnx2tf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMy-all-programs--\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSemester-4-Summer\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mInotech Intern\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mYolo-Object Detection in Android\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mObject-Detection-Android-App\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124myolov8n.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# load a custom trained model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Export the model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtflite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ShafeenYousafzai\\.conda\\envs\\venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:742\u001b[0m, in \u001b[0;36mModel.export\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    734\u001b[0m custom \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgsz\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgsz\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    740\u001b[0m }  \u001b[38;5;66;03m# method defaults\u001b[39;00m\n\u001b[0;32m    741\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcustom, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport\u001b[39m\u001b[38;5;124m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[1;32m--> 742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExporter\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ShafeenYousafzai\\.conda\\envs\\venv\\Lib\\site-packages\\ultralytics\\engine\\exporter.py:313\u001b[0m, in \u001b[0;36mExporter.__call__\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_format:  \u001b[38;5;66;03m# TensorFlow formats\u001b[39;00m\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mint8 \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m edgetpu\n\u001b[1;32m--> 313\u001b[0m     f[\u001b[38;5;241m5\u001b[39m], keras_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pb \u001b[38;5;129;01mor\u001b[39;00m tfjs:  \u001b[38;5;66;03m# pb prerequisite to tfjs\u001b[39;00m\n\u001b[0;32m    315\u001b[0m         f[\u001b[38;5;241m6\u001b[39m], _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_pb(keras_model\u001b[38;5;241m=\u001b[39mkeras_model)\n",
      "File \u001b[1;32mc:\\Users\\ShafeenYousafzai\\.conda\\envs\\venv\\Lib\\site-packages\\ultralytics\\engine\\exporter.py:142\u001b[0m, in \u001b[0;36mtry_export.<locals>.outer_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    141\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m export failure ‚ùå \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;241m.\u001b[39mt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\ShafeenYousafzai\\.conda\\envs\\venv\\Lib\\site-packages\\ultralytics\\engine\\exporter.py:137\u001b[0m, in \u001b[0;36mtry_export.<locals>.outer_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Profile() \u001b[38;5;28;01mas\u001b[39;00m dt:\n\u001b[1;32m--> 137\u001b[0m         f, model \u001b[38;5;241m=\u001b[39m \u001b[43minner_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m export success ‚úÖ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;241m.\u001b[39mt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms, saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_size(f)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f, model\n",
      "File \u001b[1;32mc:\\Users\\ShafeenYousafzai\\.conda\\envs\\venv\\Lib\\site-packages\\ultralytics\\engine\\exporter.py:848\u001b[0m, in \u001b[0;36mExporter.export_saved_model\u001b[1;34m(self, prefix)\u001b[0m\n\u001b[0;32m    840\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m starting export with tensorflow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    841\u001b[0m check_version(\n\u001b[0;32m    842\u001b[0m     tf\u001b[38;5;241m.\u001b[39m__version__,\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    846\u001b[0m     msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/ultralytics/ultralytics/issues/5161\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    847\u001b[0m )\n\u001b[1;32m--> 848\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01monnx2tf\u001b[39;00m\n\u001b[0;32m    850\u001b[0m f \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile\u001b[38;5;241m.\u001b[39msuffix, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_saved_model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mis_dir():\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'onnx2tf'"
     ]
    }
   ],
   "source": [
    "#  Export model to tflite\n",
    "\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(r'D:\\My-all-programs--\\Semester-4-Summer\\Inotech Intern\\Yolo-Object Detection in Android\\Object-Detection-Android-App\\yolov8n.pt')  # load a custom trained model\n",
    "\n",
    "# Export the model\n",
    "model.export(format='tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3aae33b-b14a-43f6-861f-3e0f2d21407f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading runs\\detect\\train3\\weights\\best_saved_model\\best_float32.tflite for TensorFlow Lite inference...\n",
      "\n",
      "image 1/1 E:\\yolov8_env\\ultralytics_android_app\\step_1_train_test_export\\test_images\\b.jpg: 640x640 3 forks, 2 spoons, 184.0ms\n",
      "Speed: 3.0ms preprocess, 184.0ms inference, 52.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'fork', 1: 'knife', 2: 'plate', 3: 'spoon'}\n",
       " obb: None\n",
       " orig_img: array([[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]], dtype=uint8)\n",
       " orig_shape: (500, 500)\n",
       " path: 'E:\\\\yolov8_env\\\\ultralytics_android_app\\\\step_1_train_test_export\\\\test_images\\\\b.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs\\\\detect\\\\predict'\n",
       " speed: {'preprocess': 3.0007362365722656, 'inference': 183.99810791015625, 'postprocess': 52.13308334350586}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction using custom tflite model\n",
    "\n",
    "#  Prediction using trained model\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLOv8n model\n",
    "model = YOLO('runs\\\\detect\\\\train3\\\\weights\\\\best_saved_model\\\\best_float32.tflite')\n",
    "\n",
    "# Run inference \n",
    "model.predict('test_images', save=True, imgsz=640, conf=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6340d01d-6c54-4989-9f04-91206888e37a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
